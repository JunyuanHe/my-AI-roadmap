å¤ªæ£’äº†ï¼ä½ å·²ç»æŠŠ**Transformer çš„åº•å±‚æœºåˆ¶**å’Œ**æœ€å°å¯è¿è¡Œ GPT**æ­èµ·æ¥äº†ï¼Œç°åœ¨æ­£å¼è¿ˆå…¥â€œè®­ç»ƒä¸€ä¸ªçœŸæ­£çš„æ¨¡å‹â€çš„é˜¶æ®µã€‚ç¬¬äºŒå‘¨æ˜¯æ•´ä¸ªè·¯çº¿ä¸­æœ€å…·å·¥ç¨‹å‘³çš„ä¸€å‘¨ï¼Œæˆ‘ä¼šæ ¹æ®ä½ ç°åœ¨çš„æ°´å¹³ï¼Œç»™ä½ ä¸€ä¸ª**é€‚åˆå®æ“ã€å¯ç›´æ¥è½åœ°çš„æŒ‡å¯¼è·¯çº¿**ã€‚

# ğŸš€ ç¬¬ 2 å‘¨ï¼šå®Œæ•´ GPT è®­ç»ƒç®¡çº¿æŒ‡å¯¼

æˆ‘ä¼šæŒ‰ç…§ä½ çš„ç›®æ ‡æ‹†è§£æˆ 4 å¤§éƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†é™„å¸¦ï¼š

âœ” èƒŒæ™¯è¦ç‚¹
âœ” ä½ éœ€è¦å®ç°çš„ä»£ç æ ¸å¿ƒ
âœ” å¸¸è§å‘
âœ” è¿›é˜¶ checklist

---

# 1ï¸âƒ£ è®­ç»ƒ BPE Tokenizerï¼ˆtokenizers åº“ï¼‰

## ğŸ“˜ èƒŒæ™¯è¦ç‚¹

GPT tokenization çš„åŸºæœ¬æµç¨‹ï¼š

1. æ”¶é›†åŸå§‹æ–‡æœ¬ â†’ UTF-8 å­—ç¬¦åºåˆ—
2. ä½¿ç”¨ BPE ç®—æ³•å­¦ä¹  merge rules
3. è¾“å‡º vocab.json + merges.txt
4. å¯é€šè¿‡ `.encode()` `.decode()` è½¬æ¢ token

âš  GPT æ¨¡å‹è®­ç»ƒæ—¶ **tokenizer æ˜¯æœ€é‡è¦çš„ç»„ä»¶ä¹‹ä¸€**ï¼Œå°¤å…¶å¯¹ä¸­æ–‡ã€‚

---

## ğŸ›  ä½ éœ€è¦å®ç°çš„å†…å®¹

### âœ” ä½¿ç”¨ HuggingFace `tokenizers` è®­ç»ƒä¸€ä¸ª BPE

```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizers = pre_tokenizers.ByteLevel()

trainer = trainers.BpeTrainer(
    vocab_size=5000,
    min_frequency=2,
    special_tokens=["<pad>", "<bos>", "<eos>", "<unk>"],
)

files = ["data/corpus.txt"]
tokenizer.train(files, trainer)
tokenizer.save("tokenizer.json")
```

### âœ” encoding æµ‹è¯•

```python
ids = tokenizer.encode("ä½ å¥½ world!").ids
```

---

## âš  å¸¸è§å‘

| é—®é¢˜          | åŸå›                        | æ¨èæ–¹æ¡ˆ             |
| ----------- | ------------------------ | ---------------- |
| ä¸­æ–‡è¢«æ‹†å¾—å¤ªç¢     | ä½¿ç”¨ ByteLevel é»˜è®¤æ‹† Unicode | è®­ç»ƒæ—¶åŠ å…¥å¤§é‡ä¸­æ–‡æ•°æ®å³å¯    |
| vocab å¤ªå°/å¤ªå¤§ | è¿‡å°ï¼šç”Ÿæˆä¹±ç ï¼›è¿‡å¤§ï¼šembedding å å‚æ•° | 3kâ€“8k å¯¹ä½ çš„å°æ¨¡å‹æœ€åˆé€‚  |
| decode é”™ä¹±   | æœªåŠ å…¥ BOS/EOS/pad          | åŠ  special tokens |

---

## ğŸ¯ Week 2 Checkpoint #1

âœ” tokenizer.json å·²è®­ç»ƒ
âœ” èƒ½ encode/decode ä»»æ„å¥å­

---

# 2ï¸âƒ£ å®ç°è®­ç»ƒè„šæœ¬ï¼ˆtrain.py çš„æ ¸å¿ƒï¼‰

è¿™é‡Œä½ è¦è¡¥é½ nanoGPT çš„è®­ç»ƒæ¥å£ï¼šbatchingã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€æ¢¯åº¦è£å‰ªã€‚

æˆ‘å¸®ä½ åˆ—ä¸€ä¸ª **éª¨æ¶ç»“æ„**ï¼ˆå¯ç›´æ¥ç…§ç€å†™ï¼‰ï¼š

```
train.py
 â”œâ”€â”€ dataset (build input tokens)
 â”œâ”€â”€ model (ä½¿ç”¨ä½ ç¬¬ 1 å‘¨å†™çš„ GPT)
 â”œâ”€â”€ optimizer (AdamW)
 â”œâ”€â”€ lr_scheduler (warmup + cosine)
 â”œâ”€â”€ training loop
```

---

# ğŸ§© 2.1 è‡ªå·±å®ç° batch sampler

è®­ç»ƒ GPT éœ€è¦æ–¹å¼ï¼š

* æŠŠ tokens åˆ‡æˆè¿ç»­çš„å¤§æ•°ç»„
* éšæœºæŠ½å–ç‰‡æ®µä½œä¸ºè¾“å…¥

ç¤ºä¾‹ï¼š

```
tokens: [a b c d e f g h i ...]
sequence_length = 256
```

batch é‡‡æ ·ï¼š

```
x = tokens[i : i+256]
y = tokens[i+1 : i+257]   # shifted target
```

---

## ğŸ›  ä»£ç è¦ç‚¹ï¼ˆä¼ªä»£ç ï¼‰

```python
def get_batch(tokens, block_size, batch_size):
    ix = torch.randint(0, len(tokens) - block_size - 1, (batch_size,))
    x = torch.stack([tokens[i:i+block_size] for i in ix])
    y = torch.stack([tokens[i+1:i+block_size+1] for i in ix])
    return x.to(device), y.to(device)
```

---

# ğŸ§© 2.2 AdamW

ä½ å¯ä»¥ç›´æ¥ç”¨ PyTorchï¼š

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95))
```

âš  GPT æ¨è betas=(0.9, 0.95)

---

# ğŸ§© 2.3 å­¦ä¹ ç‡ Warmup + Cosine Decay

GPT æ ‡å‡† scheduleï¼š

```
warmup â†’ constant lr â†’ cosine decay
```

æ¨èå®ç°ï¼š

```python
def get_lr(step):
    if step < warmup:
        return max_lr * step / warmup
    progress = (step - warmup) / (max_steps - warmup)
    return min_lr + 0.5*(max_lr-min_lr)*(1 + math.cos(math.pi * progress))
```

ç„¶åæ¯ä¸ª step æ‰‹åŠ¨æ›´æ–°ï¼š

```python
for param_group in optimizer.param_groups:
    param_group["lr"] = lr
```

---

# ğŸ§© 2.4 Gradient Clipping

éå¸¸é‡è¦ï¼Œé˜²æ­¢ loss çˆ†ç‚¸ï¼š

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
```

---

# 3ï¸âƒ£ è®­ç»ƒä¸€ä¸ª 10M å° GPTï¼ˆå…³é”®å‚æ•°ï¼‰

è¿™é‡Œç»™ä½ ä¸€ä¸ª **éå¸¸ç°å®çš„é…ç½®**ï¼Œä¿è¯èƒ½åœ¨æ™®é€š GPU ä¸Šè·‘ï¼š

| è¶…å‚         | æ¨èå€¼      |
| ---------- | -------- |
| vocab size | 3kâ€“8k    |
| n_layer    | 6        |
| n_head     | 6        |
| embed_dim  | 384      |
| block_size | 256      |
| parameters | â‰ˆ 9â€“12M  |
| batch size | 32â€“64    |
| lr         | 3e-4     |
| warmup     | 1k steps |

å¤§çº¦è®­ç»ƒ **1â€“3 å°æ—¶**ï¼ˆdepends GPUï¼‰å³å¯ loss æ”¶æ•›ã€‚

---

# ğŸ§ª è®­ç»ƒæ­¥éª¤ï¼ˆä¼ªä»£ç ï¼‰

```python
for step in range(max_steps):

    x, y = get_batch(...)
    logits = model(x)
    loss = F.cross_entropy(logits.view(-1, vocab), y.view(-1))

    loss.backward()
    clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()
    optimizer.zero_grad()

    update_lr(step)

    if step % eval_interval == 0:
        print(step, loss)
```

---

# 4ï¸âƒ£ æ¨¡å‹åˆ†æï¼šLoss / PPL / ç”Ÿæˆæ•ˆæœ

## ğŸ“‰ 4.1 Loss æ›²çº¿

ä½ éœ€è¦åšï¼š

* ä¿å­˜ `loss_log.json`
* ç”¨ python ç”» loss æ›²çº¿

Expectedï¼š

* å¼€å§‹ï¼š4â€“6
* è®­ç»ƒåï¼š1.5â€“3ï¼ˆå–å†³äºè¯­æ–™ï¼‰

---

## ğŸ“š 4.2 Perplexity

PPL = exp(loss)

```python
ppl = math.exp(eval_loss)
```

GPT å°æ¨¡å‹å…¸å‹ PPLï¼š

* éå¸¸å°çš„æ¨¡å‹ï¼ˆ10Mï¼‰ï¼š40â€“80
* è¾ƒå¥½çš„è®­ç»ƒï¼š20â€“40

---

## âœï¸ 4.3 ç”Ÿæˆæµ‹è¯•

å®Œæˆè®­ç»ƒåï¼š

```python
model.eval()
print(model.generate("ä½ å¥½", max_new_tokens=50))
```

ä½ å¤§æ¦‚ä¼šçœ‹åˆ°ï¼š

* å¥å­ç»“æ„åˆæ­¥åˆç†
* è¯­ä¹‰ç•¥æ··ä¹±
* è¯­è¨€èƒ½ä¿æŒä¸€è‡´

ä½ è®­ç»ƒä¸€å¤©åæ¨¡å‹ä¼šæ˜æ˜¾æ›´å¥½ã€‚

---

# ğŸ¯ Week 2 å®Œæˆçš„éªŒæ”¶æ ‡å‡†

æˆ‘é‡æ–°æ•´ç†æˆ checklistï¼Œæ–¹ä¾¿ä½ å¯¹æ ‡ï¼š

## âœ” **è®­ç»ƒç®¡çº¿**

* [ ] tokenizer.json è®­ç»ƒæˆåŠŸ
* [ ] train.py å¯è¿è¡Œ
* [ ] warmup + cosine decay å®ç°
* [ ] gradient clipping
* [ ] batch samplerï¼ˆæ—  dataloaderï¼‰

## âœ” **æ¨¡å‹è®­ç»ƒ**

* [ ] æˆåŠŸè®­ç»ƒä¸€ä¸ª ~10M GPT
* [ ] loss æ›²çº¿ï¼šæ”¶æ•›
* [ ] è®¡ç®— PPL

## âœ” **ç”Ÿæˆè´¨é‡**

* [ ] èƒ½ç”Ÿæˆä¸è¯­æ–™é£æ ¼ä¸€è‡´çš„æ–‡æœ¬
* [ ] èƒ½æ§åˆ¶ç”Ÿæˆé•¿åº¦

---

# ğŸ§­ å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ â€¦â€¦

ä½ æ¥ä¸‹æ¥å¸Œæœ›ä»€ä¹ˆï¼Ÿ

1. **ç»™ä½ å®Œæ•´çš„ `train.py` æ¨¡æ¿ï¼Ÿ**
2. **ç»™ä½  tokenizer + æ•°æ®å¤„ç†çš„å®Œæ•´è„šæœ¬ï¼Ÿ**
3. **å¸®ä½ å†™å›¾è¡¨å¯è§†åŒ–è„šæœ¬ï¼Ÿ**
4. **å¸®ä½ è®¾è®¡ Week 2 çš„æ¯æ—¥è®¡åˆ’ï¼Ÿ**
5. **ä½ è®­ç»ƒæ—¶é‡åˆ°çš„é—®é¢˜é€æ¡æ’æŸ¥ï¼Ÿ**

ä½ é€‰ä¸€ä¸ªï¼Œæˆ‘é©¬ä¸Šç»§ç»­å¸¦ä½ åšï¼
